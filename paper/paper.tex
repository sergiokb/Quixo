\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
%\newcommand{\hdir}{.}

\begin{document}
\English

\title
	[Thesis] % short title for page headings, not necessary if a full title fits the headings
    {Teaching LLM to play Quixo} % full title
\author
%	[S.\,M.~Kunin-Bogoiavlenskij] % short list of the authors (<= 3) for page headings, is necessary only if the full list does not fit the headings
	{S.\,M.~Kunin-Bogoiavlenskij, K.\,D.~Zhgutov, A.\,S.~Trushin} % full list of the authors, presented in the table of contetns of the issue
    [S.\,M.~Kunin-Bogoiavlenskij$^1$, K.\,D.~Zhgutov$^1$, A.\,S.~Trushin$^2$] % list of the authors presented in the title page of the article, is necessary only if it differs from the full list of the authors in braces, i.e. '{' and '}'
\email
    {kuninbogoiavlenskij@gmail.com; tarstars@yandex.ru}
%\thanks
%    {}
\organization
    {$^1$Moscow Institute of Physics and Technology;
     $^2$Yandex}
\abstract
    {	
	This paper presents a study on teaching Large Language Models (LLMs) to play the board game Quixo. 
	We explore various strategies and models to determine the most effective approach for training LLMs to excel in this strategic game. 
	Our work includes implementing MinMax algorithm, leveraging MapReduce for efficient computation, and experimenting with different language models such as YandexGPT and torch.nn.Transformer. 
	Additionally, we investigate the impact of multimodal input, including voice commands, and evaluate different notation systems for representing game moves. 
	The findings aim to provide insights into the capabilities of LLMs in strategic gameplay and the effectiveness of various training methods.

    	\textbf{Keywords}: \emph{Large Language Models; Quixo; MinMax algorithm; MapReduce}}

\maketitle
%\linenumbers

\section{Introduction}
The field of artificial intelligence has witnessed significant advancements in recent years, particularly in the development of Large Language Models (LLMs). These models have demonstrated remarkable capabilities in understanding and generating human language, making them potential candidates for solving complex tasks such as playing strategic board games. In this paper, we focus on teaching LLMs to play Quixo, a board game that requires strategic thinking and planning. We aim to understand how clever LLMs are in comprehending the spatial characteristics and strategy involved in the game.

Our study is structured around several key objectives.
 First, we aim to implement and evaluate the performance of the MinMax algorithm, which was proposed in the recent article \cite{tanak2020quixo}. 
 To handle the computational complexity, we employ MapReduce to efficiently distribute and process the calculations. Second, we train an LLM to play using the acquired full tree of win/loss game states.
Then we compare the learning capabilities of different language models, including YandexGPT, torch.nn.Transformer, and convolutional neural networks (CNNs). 
 This comparison will help us understand which models are best suited for learning the intricacies of Quixo.

Additionally, we explore the potential of multimodal input, specifically voice commands, to enhance the interaction and learning process of the LLMs. 
We also investigate various notation systems for representing game moves, including different visual and verbal methods, to determine the most effective approach for training and communication.

Through this study, we aim to contribute to the understanding of LLMs' capabilities in strategic gameplay and provide insights into effective training methods and input modalities.


\bibliographystyle{plain}
\nocite{*}
\bibliography{references}

\end{document}


